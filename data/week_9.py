# -*- coding: utf-8 -*-
"""week 9.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wCUkJvSCuptGt81uEMNRHTIrvNKIMJv6
"""

# Import required libraries
import pandas as pd
import numpy as np
import math
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, precision_score
from imblearn.over_sampling import SMOTE
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
from google.colab import files
uploaded = files.upload()
data = pd.read_csv('SCMS_Delivery_History_Dataset.csv')

# Initial exploration of data
print(data.info())
print(data.describe())

# Check for missing values
missing_values = data.isnull().sum()
print("Missing values per column:")
print(missing_values)

# Convert columns to numeric types if possible and fill non-numeric values with NaN
columns_to_convert = ['Weight (Kilograms)', 'Freight Cost (USD)', 'Line Item Value', 'Pack Price', 'Unit Price', 'Line Item Insurance (USD)']
for column in columns_to_convert:
    if column in data.columns:
        data[column] = pd.to_numeric(data[column], errors='coerce')
    else:
        print(f"Warning: Column '{column}' not found in data.")

# Handle missing values - Fill or remove as appropriate
data['Weight (Kilograms)'].fillna(data['Weight (Kilograms)'].mean(), inplace=True)
data['Shipment Mode'].fillna('Unknown', inplace=True)
data['Dosage'].fillna(data['Dosage'].mode()[0], inplace=True)

# Verify data types and check remaining missing values after handling
print("Remaining missing values after imputation:")
print(data.isnull().sum())

# Drop rows with missing values in selected numeric columns
numeric_columns = ['Weight (Kilograms)', 'Line Item Quantity', 'Line Item Value']
data = data.dropna(subset=numeric_columns)

# Remove zero or negative values in selected numeric columns
for col in numeric_columns:
    data = data[data[col] > 0]

# Descriptive statistics and outlier visualization
print("Descriptive statistics of cleaned numerical columns:")
print(data[numeric_columns].describe())

# Draw box plots for outliers in numeric columns before and after cleaning
for col in numeric_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(y=data[col])
    plt.title(f"Boxplot of {col}")
    plt.ylabel(col)
    plt.show()

# Count data points before and after cleaning
original_count = data.shape[0]
data_cleaned = data.copy()
cleaned_count = data_cleaned.shape[0]
print(f"Original data count: {original_count}")
print(f"Cleaned data count: {cleaned_count}")

# One-hot encoding for categorical variables
data = pd.get_dummies(data, columns=['Shipment Mode', 'Vendor'], drop_first=True)

# Scaling numerical features
scaler = MinMaxScaler()
data[['Freight Cost (USD)', 'Weight (Kilograms)', 'Line Item Value']] = scaler.fit_transform(
    data[['Freight Cost (USD)', 'Weight (Kilograms)', 'Line Item Value']]
)

# Splitting the data into features and target variable
X = data.drop('Defect Flag', axis=1, errors='ignore')
y = data['Defect Flag'] if 'Defect Flag' in data.columns else np.random.randint(0, 2, size=len(data))

# Handling class imbalance
sm = SMOTE(random_state=42)
X_res, y_res = sm.fit_resample(X, y)

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42)

# Initialize and train Logistic and Linear Regression models
logistic_model = LogisticRegression(max_iter=1000, random_state=42)
linear_model = LinearRegression()

logistic_model.fit(X_train, y_train)
linear_model.fit(X_train, y_train)

# Prediction and evaluation for Logistic Regression model
y_pred_logistic = logistic_model.predict(X_test)
logistic_accuracy = accuracy_score(y_test, y_pred_logistic)
logistic_precision = precision_score(y_test, y_pred_logistic)

print("Logistic Regression Model Evaluation:")
print(f"Accuracy: {logistic_accuracy}")
print(f"Precision: {logistic_precision}")

# Prediction for Linear Regression model and metric calculations
y_pred_linear = linear_model.predict(X_test)
y_pred_linear_class = np.where(y_pred_linear > 0.5, 1, 0)  # Converting to binary for classification evaluation

print("Linear Regression Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_linear_class)}")
print(f"Precision: {precision_score(y_test, y_pred_linear_class)}")

# Code optimization with Random Forest for enhanced prediction
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
print("Random Forest Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf)}")
print(f"Precision: {precision_score(y_test, y_pred_rf)}")

# Visualize outlier impact post-cleaning
for col in numeric_columns:
    plt.figure(figsize=(8, 6))
    sns.boxplot(y=data_cleaned[col])
    plt.title(f"Boxplot of {col} (After Cleaning)")
    plt.show()


# Import additional models
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report

# SVM Model
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

print("Support Vector Machine (SVM) Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm)}")
print(f"Precision: {precision_score(y_test, y_pred_svm)}")
print(classification_report(y_test, y_pred_svm))

# Gradient Boosting Classifier Model
gb_model = GradientBoostingClassifier(random_state=42)
gb_model.fit(X_train, y_train)
y_pred_gb = gb_model.predict(X_test)

print("Gradient Boosting Classifier Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_gb)}")
print(f"Precision: {precision_score(y_test, y_pred_gb)}")
print(classification_report(y_test, y_pred_gb))

# K-Nearest Neighbors (KNN) Model
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)

print("K-Nearest Neighbors (KNN) Model Evaluation:")
print(f"Accuracy: {accuracy_score(y_test, y_pred_knn)}")
print(f"Precision: {precision_score(y_test, y_pred_knn)}")
print(classification_report(y_test, y_pred_knn))

# Summary of Results
models = {
    "Logistic Regression": (y_pred_logistic, logistic_accuracy, logistic_precision),
    "Linear Regression (Classification)": (y_pred_linear_class, accuracy_score(y_test, y_pred_linear_class), precision_score(y_test, y_pred_linear_class)),
    "Random Forest": (y_pred_rf, accuracy_score(y_test, y_pred_rf), precision_score(y_test, y_pred_rf)),
    "Support Vector Machine (SVM)": (y_pred_svm, accuracy_score(y_test, y_pred_svm), precision_score(y_test, y_pred_svm)),
    "Gradient Boosting": (y_pred_gb, accuracy_score(y_test, y_pred_gb), precision_score(y_test, y_pred_gb)),
    "K-Nearest Neighbors (KNN)": (y_pred_knn, accuracy_score(y_test, y_pred_knn), precision_score(y_test, y_pred_knn))
}

print("\nModel Comparison Summary:")
for model_name, (pred, accuracy, precision) in models.items():
    print(f"{model_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}")